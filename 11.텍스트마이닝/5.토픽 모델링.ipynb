{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "stm = PorterStemmer() #어근 추출\n",
    "\n",
    "stopwords = set(stopwords.words('english')) #불용어 집합\n",
    "#첫글자는 알파벳으로 시작, 두번째부터는 알파벳,숫자,-_.만 허용\n",
    "pattern=re.compile('[a-zA-Z][-_a-zA-Z0-9.]*') \n",
    "\n",
    "def tokenize(sentence):\n",
    "    def stem(w):\n",
    "        try: return stem.stem(w) #어근추출\n",
    "        except: return w #예외가 발생하면 그대로 리턴\n",
    "    #숫자로 바꾼 후 단어 구분, 불용어 제거, 패턴에 맞는 단어만 선택\n",
    "    return [stem(w) for w in word_tokenize(sentence.lower())\n",
    "           if w not in stopwords and pattern.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chief',\n",
       " 'justice',\n",
       " 'roberts',\n",
       " 'president',\n",
       " 'carter',\n",
       " 'president',\n",
       " 'clitno',\n",
       " 'president',\n",
       " 'bush',\n",
       " 'president',\n",
       " 'obama',\n",
       " 'yeongwoo',\n",
       " 'dfdfdf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='Chief Justice Roberts, President Carter, President Clitno, President Bush, President Obama, Yeongwoo,!@dfdfdf'\n",
    "\n",
    "tokenize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "#k 토픽갯수, min_cf=5 5회 미만 등장한 단어들은 제거\n",
    "model=tp.LDAModel(k=20, min_cf=5) #토픽모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Chief Jus\n",
      "Total docs: 1\n",
      "Total words: 146\n",
      "Vocab size: 21\n"
     ]
    }
   ],
   "source": [
    "#파일 오픈 i 인덱스, line 한 라인\n",
    "for i,line in enumerate(open('d:/data/text/trumph.txt',encoding='ms949')):\n",
    "    print(i,line[:10])\n",
    "    model.add_doc(tokenize(line)) #단어별로 나누어 모형에 추가\n",
    "     \n",
    "model.train(0) # 0 학습횟수, 아직 학습이 시작된 상태가 아님\n",
    "print('Total docs:',len(model.docs))\n",
    "print('Total words:',model.num_words)\n",
    "print('Vocab size:', model.num_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\tone, world, dreams, america, people, american, country, every, nation, never\n",
      "Topic #1\tamerica, people, american, country, one, every, nation, never, back, world\n",
      "Topic #2\tamerica, never, today, people, american, country, one, every, nation, back\n",
      "Topic #3\tamerica, people, american, country, one, every, nation, never, back, world\n",
      "Topic #4\tamerican, america, people, country, one, every, nation, never, back, world\n",
      "Topic #5\tcountry, back, many, america, people, american, one, every, nation, never\n",
      "Topic #6\tnation, god, today, america, people, american, country, one, every, never\n",
      "Topic #7\tamerica, people, american, country, one, every, nation, never, back, world\n",
      "Topic #8\tevery, america, people, american, country, one, nation, never, back, world\n",
      "Topic #9\tamerica, people, american, country, one, every, nation, never, back, world\n",
      "Topic #10\tgreat, people, america, american, country, one, every, nation, never, back\n",
      "Topic #11\tamerica, people, american, country, one, every, nation, never, back, world\n",
      "Topic #12\tcountry, today, america, people, american, one, every, nation, never, back\n",
      "Topic #13\tamerica, people, american, country, one, every, nation, never, back, world\n",
      "Topic #14\tprotected, across, america, people, american, country, one, every, nation, never\n",
      "Topic #15\tamerica, people, american, country, one, every, nation, never, back, world\n",
      "Topic #16\tpeople, god, america, american, country, one, every, nation, never, back\n",
      "Topic #17\tright, america, people, american, country, one, every, nation, never, back\n",
      "Topic #18\tamerica, make, president, people, american, country, one, every, nation, never\n",
      "Topic #19\tnew, america, people, american, country, one, every, nation, never, back\n"
     ]
    }
   ],
   "source": [
    "model.train(200) #200회 학습\n",
    "for i in range(model.k):\n",
    "    res=model.get_topic_words(i,top_n=10)\n",
    "    print('Topic #{0}'.format(i),end='\\t')\n",
    "    print(', '.join(w for w,p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "emma_raw=nltk.corpus.gutenberg.raw('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 1\n",
      "Total words: 63740\n",
      "Vocab size: 2192\n"
     ]
    }
   ],
   "source": [
    "model=tp.LDAModel(k=5, min_cf=5)\n",
    "model.add_doc(tokenize(emma_raw))\n",
    "\n",
    "model.train(0) # 0 학습횟수, 아직 학습이 시작된 상태가 아님\n",
    "print('Total docs:',len(model.docs))\n",
    "print('Total words:',model.num_words)\n",
    "print('Vocab size:', model.num_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\tcould, know, said, much, never, always, harriet, quite, jane, friend\n",
      "Topic #1\twould, mr., might, woodhouse, every, mrs., man, one, thought, little\n",
      "Topic #2\tmust, weston, thing, say, harriet, good, great, mr., dear, see\n",
      "Topic #3\tmiss, time, mrs., one, indeed, body, day, give, long, hope\n",
      "Topic #4\temma, mr., think, knightley, nothing, elton, fairfax, much, would, young\n"
     ]
    }
   ],
   "source": [
    "model.train(100) #200회 학습\n",
    "for i in range(model.k):\n",
    "    res=model.get_topic_words(i,top_n=10)\n",
    "    print('Topic #{0}'.format(i),end='\\t')\n",
    "    print(', '.join(w for w,p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
